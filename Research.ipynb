{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI-Powered Research Assistant\n",
    "\n",
    "#### Problem Statement\n",
    "Traditional research workflows often face several limitations:\n",
    "- Lack of depth in information gathering and analysis\n",
    "- Inconsistent research methodology across different topics\n",
    "- Limited context retention during complex research tasks\n",
    "- Manual effort in synthesizing information from multiple sources\n",
    "- Difficulty in maintaining research coherence across multiple subtopics\n",
    "\n",
    "This notebook presents an innovative solution that overcomes these limitations by implementing an advanced research orchestration system.\n",
    "\n",
    "#### Solution Architecture\n",
    "Our implementation leverages a coordinated group of Large Language Models (LLMs), each specialized for different aspects of the research process:\n",
    "\n",
    "1. **Research Planner (Deepseek)**: \n",
    "   - Analyzes research queries\n",
    "   - Breaks down complex topics into structured subtasks\n",
    "   - Creates detailed research execution plans\n",
    "\n",
    "2. **Research Executor (Claude)**:\n",
    "   - Executes the planned research steps\n",
    "   - Manages tool interactions\n",
    "   - Synthesizes information from multiple sources\n",
    "\n",
    "3. **Information Retrieval (Perplexity AI)**:\n",
    "   - Performs targeted web searches\n",
    "   - Extracts relevant information\n",
    "   - Maintains citation tracking\n",
    "\n",
    "#### Current Implementation\n",
    "This notebook implements the core research flow with:\n",
    "- Multi-LLM orchestration\n",
    "- Structured research planning\n",
    "- Web-based information retrieval\n",
    "- Citation management\n",
    "- Research synthesis and reporting\n",
    "\n",
    "#### Future Improvements\n",
    "The system is designed to be enhanced with RAG (Retrieval-Augmented Generation) capabilities:\n",
    "- Integration of domain-specific knowledge bases\n",
    "- Custom vector stores for improved context retention\n",
    "- Semantic search for better information retrieval\n",
    "- Dynamic document chunking and embedding\n",
    "- Hybrid search combining dense and sparse retrievers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install litellm openai anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Import necessary libraries and configure API access. This implementation requires several API keys for different LLM providers, each serving a specific purpose in the research pipeline.\n",
    "\n",
    "### Required API Keys\n",
    "Configure these in your `.env` file:\n",
    "\n",
    "```env\n",
    "# Core LLM Providers\n",
    "ANTHROPIC_API_KEY=      # For Claude models (tool execution)\n",
    "OPENAI_API_KEY=         # For GPT models (optional alternative)\n",
    "OPENROUTER_API_KEY=     # For accessing multiple models through a single endpoint\n",
    "PERPLEXITYAI_API_KEY=   # For web search and information retrieval\n",
    "DEEPSEEK_API_KEY=       # For planning and reasoning (if accessing directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from litellm import completion\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "anthropic_client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Prompts and Configuration\n",
    "Define the core prompt template for the Deepseek model. This prompt instructs the model to:\n",
    "1. Act as an expert researcher\n",
    "2. Process complex research tasks\n",
    "3. Create detailed analysis plans\n",
    "4. Work with an LLM agent for plan execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_prompt = \"\"\"\n",
    "You are an expert researcher with deep expertise in finance, legal, and tax matters.\n",
    "The first input you will receive will be a complex Research task that needs to be carefully reasoned through to solve. \n",
    "Your task is to review the challenge, conduct thorough research, and create a detailed plan to analyze information, assess implications, and provide comprehensive insights.\n",
    "\n",
    "You will have access to an LLM agent that is responsible for executing the plan that you create and will return results.\n",
    "\n",
    "The LLM agent has access to the following functions:\n",
    "    - search_web(Question)\n",
    "        - This function performs a web search and returns relevant information based on the provided query\n",
    "        \n",
    "When creating a plan for the LLM to execute, break your instructions into a logical, step-by-step order, using the specified format:\n",
    "    - **Main actions are numbered** (e.g., 1, 2, 3).\n",
    "    - **Sub-actions are lettered** under their relevant main actions (e.g., 1a, 1b).\n",
    "        - **Sub-actions should start on new lines**\n",
    "    - **Specify conditions using clear 'if...then...else' statements** (e.g., 'If the financial statement shows a profit, then...').\n",
    "    - **For actions that require using one of the above functions defined**, write a step to call a function using backticks for the function name (e.g., `call the fetch_context function`).\n",
    "        - Ensure that the proper input arguments are given to the model for instruction. There should not be any ambiguity in the inputs.\n",
    "    - **The last step** in the instructions should always be calling the `instructions_complete` function. This is necessary so we know the LLM has completed all of the instructions you have given it.\n",
    "    - **Detailed steps** The plan generated must be extremely detailed and thorough with explanations at every step.\n",
    "Use markdown format when generating the plan with each step and sub-step.\n",
    "\n",
    "Please find the scenario below.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions: Planning\n",
    "\n",
    "The `call_planner` function serves as a flexible interface for generating structured research plans. It can be configured to work with various reasoning-focused language models:\n",
    "\n",
    "### Model Options\n",
    "- **Reasoning Models**:\n",
    "  - Deepseek (current implementation)\n",
    "  - o1-mini\n",
    "  - qwen/qwq-32b-preview\n",
    "  - gemini-2.0-flash-thinking-exp:free\n",
    "  - Other models optimized for reasoning and planning\n",
    "\n",
    "### API Integration Options\n",
    "- **Direct API Access**:\n",
    "  - OpenAI Client (currently used)\n",
    "  - Anthropic Client\n",
    "  - Custom API implementations\n",
    "\n",
    "- **API Aggregators**:\n",
    "  - OpenRouter\n",
    "  - LiteLLM\n",
    "  - Other model routing services\n",
    "\n",
    "The function maintains a consistent interface regardless of the underlying model choice, making it easy to swap models based on:\n",
    "- Cost considerations\n",
    "- Performance requirements\n",
    "- Availability in different regions\n",
    "- Specific reasoning capabilities needed\n",
    "\n",
    "The current implementation uses Deepseek through OpenAI, but the architecture is designed to be model-agnostic for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_planner(scenario):\n",
    "    prompt = f\"\"\"\n",
    "    {deepseek_prompt}\n",
    "        \n",
    "    Scenario:\n",
    "    {scenario}\n",
    "\n",
    "    Please provide the next steps in your plan.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-reasoner\",\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "    )\n",
    "\n",
    "    plan = response.choices[0].message.content\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message Management\n",
    "Helper function to manage and display various types of messages including:\n",
    "- Status updates\n",
    "- Research plans\n",
    "- Assistant responses\n",
    "- Function calls and their responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_message(message_list, message):\n",
    "    message_list.append(message)\n",
    "    message_type = message.get('type', '')\n",
    "    if message_type == 'status':\n",
    "        print(message['message'])\n",
    "    elif message_type == 'plan':\n",
    "        print(\"\\nPlan:\\n\", message['content'])\n",
    "    elif message_type == 'assistant':\n",
    "        print(\"\\nAssistant:\\n\", message['content'])\n",
    "    elif message_type == 'function_call':\n",
    "        print(f\"\\nFunction call: {message['function_name']} with arguments {message['arguments']}\")\n",
    "    elif message_type == 'function_response':\n",
    "        print(f\"\\nFunction response for {message['function_name']}: {message['response']}\")\n",
    "    else:\n",
    "        print(message.get('content', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search Implementation\n",
    "Implementation of the web search functionality using Perplexity AI API:\n",
    "- Performs web searches with academic rigor\n",
    "- Processes and formats citations\n",
    "- Handles error cases gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query):\n",
    "    \"\"\"\n",
    "    Function to search the web using Perplexity AI API and return the answer\n",
    "    with inline citation links instead of numbered references.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to be processed\n",
    "        \n",
    "    Returns:\n",
    "        str: The response content with inline links replacing citations like [1], [2], etc.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.environ[\"PERPLEXITYAI_API_KEY\"], base_url=\"https://api.perplexity.ai\")\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an artificial intelligence assistant and you need to answer like a Data collection engine with as much information as possible.\",\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"sonar\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"sonar\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        \n",
    "        if not response.choices:\n",
    "            logger.error(\"No response choices available\")\n",
    "            return \"Error: No response received from the search\"\n",
    "            \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Check if citations exist in the response\n",
    "        citations = getattr(response, 'citations', None)\n",
    "        \n",
    "        # If no citations or citations is empty, return content as is\n",
    "        if not citations:\n",
    "            return content\n",
    "            \n",
    "        # Process citations if they exist\n",
    "        def replace_citation_with_link(match):\n",
    "            citation_num_str = match.group(0)[1:-1]\n",
    "            try:\n",
    "                citation_idx = int(citation_num_str) - 1\n",
    "                if citation_idx < 0 or citation_idx >= len(citations):\n",
    "                    return match.group(0)\n",
    "                return f\"({citations[citation_idx]})\"\n",
    "            except (ValueError, IndexError):\n",
    "                return match.group(0)\n",
    "        \n",
    "        content_with_links = re.sub(r'\\[\\d+\\]', replace_citation_with_link, content)\n",
    "        return content_with_links\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during web search: {str(e)}\")\n",
    "        return f\"Error during search: {str(e)}\"\n",
    "\n",
    "\n",
    "def instructions_complete(final_report):\n",
    "    return f\"Final Report: {final_report}\"\n",
    "\n",
    "\n",
    "function_mapping = {\n",
    "    'search_web': search_web,\n",
    "    'instructions_complete': instructions_complete\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Definitions\n",
    "Define the available tools and their schemas for the LLM to use:\n",
    "- search_web: For web-based research\n",
    "- instructions_complete: For finalizing research tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = [\n",
    "    {\n",
    "        \"name\": \"search_web\",\n",
    "        \"description\": \"Function performs a web search and returns relevant information based on the provided query\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The search query to be processed\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"instructions_complete\",\n",
    "        \"description\": \"Function should be called when we have completed ALL of the instructions.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"final_report\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Final Report based on the analysis.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"final_report\"],\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assistant Configuration\n",
    "Define the core behavior and responsibilities of the research assistant:\n",
    "- Policy execution\n",
    "- Decision-making process\n",
    "- Action execution flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant responsible for executing the policy on handling deep research tasks. \n",
    "Your task is to follow the policy exactly as it is written and perform the necessary actions.\n",
    "\n",
    "You must explain your decision-making process across various steps.\n",
    "\n",
    "# Steps\n",
    "1. **Read and Understand Policy**: Carefully read and fully understand the given policy on Deep research Task.\n",
    "2. **Identify the exact step in the policy**: Determine which step in the policy you are at, and execute the instructions according to the policy.\n",
    "3. **Decision Making**: Briefly explain your actions and why you are performing them.\n",
    "4. **Action Execution**: Perform the actions required by calling any relevant functions and input parameters. \n",
    "\n",
    "POLICY:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Processing\n",
    "Helper function to parse and process responses from the Anthropic API:\n",
    "- Extracts text content\n",
    "- Identifies tool usage\n",
    "- Manages response structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan Execution\n",
    "Core function that executes the generated research plan:\n",
    "- Manages conversation flow with Claude\n",
    "- Handles tool calls\n",
    "- Processes responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_execute(message_list, plan):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': plan}\n",
    "    ]\n",
    "    \n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=1024,\n",
    "        system=system_prompt,\n",
    "        tools=TOOLS,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "    final_answer = \"\"\n",
    " \n",
    "    while response.stop_reason == \"tool_use\":\n",
    "        if response.content and len(response.content) > 0:\n",
    "            text_block = response.content[0]\n",
    "            if hasattr(text_block, 'text') and text_block.text:\n",
    "                print(\"\\nModel Response:\", text_block.text)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        tool = response.content[-1]\n",
    "        print(\"\\nCalling Tool:\", tool.name)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        if (tool.name and tool.name == 'instructions_complete'):\n",
    "            final_answer = tool.input\n",
    "            print(\"\\n\", final_answer)\n",
    "            print(\"-\" * 80)\n",
    "            break\n",
    "\n",
    "        if tool.name in function_mapping:\n",
    "            input_arguments_str = tool.input\n",
    "            print(\"\\nInput Arguments:\", input_arguments_str)\n",
    "            print(\"-\" * 80)\n",
    "            res = function_mapping[tool.name](**input_arguments_str)\n",
    "            print(\"\\nTool Response:\", res)\n",
    "            print(\"-\" * 80)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tool: {tool.name}\")\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": [{\n",
    "            \"type\": \"tool_result\",\n",
    "            \"tool_use_id\": tool.id,\n",
    "            \"content\": res\n",
    "        }]})\n",
    "\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=1024,\n",
    "            system=system_prompt,\n",
    "            tools=TOOLS,\n",
    "            messages=messages\n",
    "        )\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario Processing\n",
    "Main orchestration function that:\n",
    "- Generates research plans\n",
    "- Executes plans\n",
    "- Manages message flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenario(message_list, scenario):\n",
    "    append_message(message_list, {'type': 'status', 'message': 'Generating plan...'})\n",
    "\n",
    "    plan = call_planner(scenario)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    append_message(message_list, {'type': 'plan', 'content': plan})\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    append_message(message_list, {'type': 'status', 'message': 'Executing plan...'})\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    messages = plan_execute(message_list, plan)\n",
    "\n",
    "    append_message(message_list, {'type': 'status', 'message': 'Processing complete.'})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research Outline Generation\n",
    "Specialized function for creating structured research outlines:\n",
    "- Analyzes scenarios\n",
    "- Extracts key themes\n",
    "- Generates hierarchical outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outline(messages, scenario):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a research assistant specialized in creating focused research outlines.\n",
    "    Your role is to:\n",
    "    1. Analyze the provided scenario/question\n",
    "    2. Extract key research themes directly related to the question\n",
    "    3. Generate a hierarchical outline covering only the relevant themes\n",
    "    4. Use markdown format with clear heading levels (###, -, *)\n",
    "    5. Ensure every heading directly addresses components of the research question\n",
    "\n",
    "    Do not include:\n",
    "    - Methodological steps (like data collection or verification)\n",
    "    - Process-related headings (like \"Final Steps\")\n",
    "    - Any content not directly answering the research question\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Based on the following research scenario and messages, create a focused outline that directly addresses the key components of the question.\n",
    "\n",
    "    <Research>\n",
    "        {scenario}\n",
    "    </Research>\n",
    "    \n",
    "    <message>\n",
    "        {messages}\n",
    "    </message>\n",
    "\n",
    "    REQUIREMENTS:\n",
    "    1. Use markdown formatting\n",
    "    2. Include only headings relevant to answering the scenario\n",
    "    3. Structure as main headings (###) and subheadings (-)\n",
    "    4. Ensure each heading connects to either:\n",
    "    - Factors affecting Indian stock market performance\n",
    "    - Investment approaches of experienced Indian investors\n",
    "\n",
    "    OUTPUT FORMAT:\n",
    "    ### [Main Topic]\n",
    "    - [Subtopic]\n",
    "    - [Subtopic]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Report Generation\n",
    "Specialized function for creating detailed, structured reports:\n",
    "- Analyzes messages and research context\n",
    "- Generates a report that precisely follows the given outline\n",
    "- Provides in-depth analysis of research data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_report(messages, scenario, outline):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a professional report writer specializing in detailed research analysis and structured reporting.\n",
    "    Your primary responsibility is to generate comprehensive reports that precisely follow given outline structures while providing in-depth analysis of research data.\n",
    "\n",
    "    Key Responsibilities:\n",
    "    - Strictly adhere to provided outline structures\n",
    "    - Generate detailed content for each outline section and subsection\n",
    "    - Ensure thorough coverage of all points in the outline\n",
    "    - Maintain consistent depth of analysis across all sections\n",
    "    - Support all findings with specific evidence from research data\n",
    "    - Properly cite all sources using URLs in the format [Source](URL)\n",
    "    - When referencing information, always include the relevant citation\n",
    "    - Maintain a consistent citation style throughout the report\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a comprehensive, detailed report following the exact structure of the provided outline.\n",
    "    Each section must be thoroughly developed with supporting evidence from the research data.\n",
    "\n",
    "    <Research Context>\n",
    "        {scenario}\n",
    "    </Research Context>\n",
    "\n",
    "    <Research Structure>\n",
    "        {outline}\n",
    "    </Research Structure>\n",
    "\n",
    "    <Research Data>\n",
    "        {messages}\n",
    "    </Research Data>\n",
    "\n",
    "    Report Requirements:\n",
    "    1. STRICTLY FOLLOW THE PROVIDED OUTLINE:\n",
    "    - Generate content for every section and subsection in the outline\n",
    "    - Maintain the exact hierarchy and organization specified\n",
    "    - Use consistent heading levels that match the outline structure\n",
    "    - Ensure no outline points are skipped or merged\n",
    "\n",
    "    2. DETAIL AND EVIDENCE:\n",
    "    - Provide extensive detail for each outline point\n",
    "    - Include multiple supporting examples from research data\n",
    "    - Quote relevant passages from the research materials\n",
    "    - Analyze each point thoroughly before moving to the next\n",
    "\n",
    "    3. FORMATTING AND STRUCTURE:\n",
    "    - Use markdown headers that match outline levels (# for main sections, ## for subsections, etc.)\n",
    "    - Include transitional text between sections to maintain flow\n",
    "    - Format quotes and evidence appropriately\n",
    "    - Maintain consistent depth across all sections\n",
    "\n",
    "    4. ANALYSIS REQUIREMENTS:\n",
    "    - Provide data-driven insights for each section\n",
    "    - Include metrics and quantitative analysis where applicable\n",
    "    - Draw connections between related findings\n",
    "    - Support each conclusion with specific evidence and citations\n",
    "    - When citing multiple sources for a claim, include all relevant URLs\n",
    "\n",
    "    5. CITATION FORMAT:\n",
    "    - Use markdown links for citations: [Source](URL)\n",
    "    - Citations should be placed immediately after the relevant information\n",
    "    - Multiple citations should be separated by commas\n",
    "    - Ensure every URL is valid and properly formatted\n",
    "\n",
    "    Generate the report now, beginning with the first outline point and maintaining strict adherence to the outline structure.\n",
    "    Remember to include proper citations for all information and a complete references section at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    response = completion(\n",
    "        model=\"openrouter/minimax/minimax-01\",\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': user_prompt}\n",
    "        ]\n",
    "    )               \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def clean_markdown(content):\n",
    "    return content.replace('```markdown', '').replace('```', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage\n",
    "Demonstration of the research system with a real-world scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"Provide a small report on the DPDA Regulations.\"\n",
    "\n",
    "messages = process_scenario([], scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline = generate_outline(messages, scenario)\n",
    "print(outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report = get_final_report(messages, scenario, outline)\n",
    "clean_final_report = clean_markdown(final_report)\n",
    "print(clean_final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_assistant_content(content):\n",
    "    result = {\n",
    "        'text_content': [],\n",
    "        'tool_inputs': []\n",
    "    }\n",
    "    \n",
    "    if isinstance(content, list):\n",
    "        for block in content:\n",
    "            block_str = str(block)\n",
    "            \n",
    "            if 'TextBlock' in block_str:\n",
    "                # Extract text from TextBlock\n",
    "                try:\n",
    "                    start_idx = block_str.find('text=\"') + 6\n",
    "                    end_idx = block_str.find('\", type=')\n",
    "                    if start_idx != -1 and end_idx != -1:\n",
    "                        text_content = block_str[start_idx:end_idx]\n",
    "                        result['text_content'].append(text_content)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting text: {e}\")\n",
    "                    \n",
    "            elif 'ToolUseBlock' in block_str:\n",
    "                # Extract input from ToolUseBlock\n",
    "                try:\n",
    "                    start_idx = block_str.find(\"input=\") + 6\n",
    "                    end_idx = block_str.find(\", name=\")\n",
    "                    if start_idx != -1 and end_idx != -1:\n",
    "                        input_str = block_str[start_idx:end_idx]\n",
    "                        # Using string manipulation instead of ast.literal_eval\n",
    "                        if 'query' in input_str:\n",
    "                            query_start = input_str.find(\"'query': '\") + 9\n",
    "                            query_end = input_str.find(\"'}\", query_start)\n",
    "                            if query_start != -1 and query_end != -1:\n",
    "                                query = input_str[query_start:query_end]\n",
    "                                result['tool_inputs'].append({'query': query})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting tool input: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_tool_result_content(content):\n",
    "    if isinstance(content, list) and len(content) > 0:\n",
    "        for item in content:\n",
    "            if isinstance(item, dict) and item.get('type') == 'tool_result':\n",
    "                return f\"Tool Result ID: {item.get('tool_use_id', 'N/A')}\\nContent:\\n{item.get('content', '')}\"\n",
    "    return str(content)\n",
    "\n",
    "def format_messages(messages):\n",
    "    formatted_output = []\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message.get('role', '')\n",
    "        content = message.get('content', '')\n",
    "        \n",
    "        formatted_parts = [f\"Role: {role}\"]\n",
    "        \n",
    "        if role == 'assistant' and isinstance(content, list):\n",
    "            # Extract and format assistant content\n",
    "            extracted = extract_from_assistant_content(content)\n",
    "            \n",
    "            # Add text content\n",
    "            if extracted['text_content']:\n",
    "                formatted_parts.append(\"Text Content:\")\n",
    "                for text in extracted['text_content']:\n",
    "                    formatted_parts.append(text)\n",
    "            \n",
    "            # Add tool inputs\n",
    "            if extracted['tool_inputs']:\n",
    "                formatted_parts.append(\"\\nTool Inputs:\")\n",
    "                for tool_input in extracted['tool_inputs']:\n",
    "                    formatted_parts.append(str(tool_input))\n",
    "        else:\n",
    "            # For user messages, check if it contains tool_result\n",
    "            if isinstance(content, list) and any(isinstance(item, dict) and item.get('type') == 'tool_result' for item in content):\n",
    "                formatted_parts.append(extract_tool_result_content(content))\n",
    "            else:\n",
    "                formatted_parts.append(f\"Content: {content}\")\n",
    "        \n",
    "        # Join all parts with newlines and add separator\n",
    "        formatted_message = \"\\n\".join(formatted_parts) + f\"\\n{'='*50}\"\n",
    "        formatted_output.append(formatted_message)\n",
    "    \n",
    "    # Join all formatted messages with newlines\n",
    "    return \"\\n\".join(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_message_string = format_messages(messages)\n",
    "print(formatted_message_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = f\"\"\"\n",
    "{formatted_message_string}\n",
    "\n",
    "{clean_final_report}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
