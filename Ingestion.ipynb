{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import fitz\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "from litellm import completion\n",
    "from PIL import Image\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_base64_images(pdf_path):\n",
    "    base64_images = []\n",
    "\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    def process_page(page):\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        buffered = io.BytesIO()\n",
    "        img.save(buffered, format=\"PNG\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        pages = [pdf_document[i] for i in range(len(pdf_document))]\n",
    "        base64_images = list(executor.map(process_page, pages))\n",
    "    pdf_document.close()\n",
    "    return base64_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summary_from_image(base64_str, max_retries=3, retry_delay=5):\n",
    "    message = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "    You will be given an image containing text. Your task is to accurately transcribe all the text from this image. \n",
    "    Pay special attention to names, tables and numbers.\n",
    "\n",
    "    Follow these steps to complete the task:\n",
    "    1. Carefully examine the entire image.\n",
    "    2. Transcribe all visible text exactly as it appears in the image.\n",
    "    3. If any text is unclear or illegible, do not attempt to guess or fill in information. Instead, indicate unclear text with [unclear] in your transcription.\n",
    "    4. Pay particular attention to visual elements such as tables, charts, and diagrams. Ensure these are transcribed accurately and in a clear, organized manner.\n",
    "    5. If the order of information in the image is not clear, think step by step about the logical flow of the content. Arrange the transcribed information in a relevant and coherent order.\n",
    "    6. Do not add any information that is not present in the image.\n",
    "    7. Do not include any preamble or explanation about the transcription process in your response.\n",
    "    8. For Visual Elements:\n",
    "        a. For tables: Transcribe headers, rows, and columns in a markdown table format, ensuring proper alignment and structure.\n",
    "        b. For charts or diagrams: Provide a detailed description of the type (e.g., bar chart, flowchart), layout, and any labeled data points.\n",
    "        Example Markdown Table:\n",
    "        | Column 1 Header | Column 2 Header | Column 3 Header |\n",
    "        |---------------- |-----------------|-----------------|\n",
    "        | Row 1, Cell 1   | Row 1, Cell 2   | Row 1, Cell 3   |\n",
    "        | Row 2, Cell 1   | Row 2, Cell 2   | Row 2, Cell 3   |\n",
    "        | Row 3, Cell 1   | Row 3, Cell 2   | Row 3, Cell 3   |\n",
    "    9. Your response should only contain the transcribed content from the image, organized in a logical manner if necessary.\n",
    "    10. If you encounter any issues or if the image is not clear enough to transcribe, explain the problem instead of providing a transcription.\n",
    "    \"\"\"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_str}\"\n",
    "            }},\n",
    "            {\"type\": \"text\", \"text\": \"Please transcribe all the text from this image, ensuring the data is in markdown format.\"}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = completion(\n",
    "                model=\"openrouter/google/gemini-flash-1.5\",\n",
    "                messages=message,\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"APIError occurred: {str(e)}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"Max retries reached. Unable to process image.\")\n",
    "                return f\"Error processing image: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_to_summary(base64_images):\n",
    "    def process_image_with_index(args):\n",
    "        index, base64_str = args\n",
    "        processed_summary = process_summary_from_image(base64_str)\n",
    "        return index, processed_summary\n",
    "\n",
    "    summary = [None] * len(base64_images)\n",
    "    with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "        futures = [executor.submit(process_image_with_index, (i, img)) for i, img in enumerate(base64_images)]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(base64_images), desc=\"Processing images\"):\n",
    "            index, processed_summary = future.result()\n",
    "            summary[index] = processed_summary\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def prepare_data_for_ingestion(summary, file_name):\n",
    "    contents = []\n",
    "\n",
    "    for page_num, result in enumerate(summary, start=1):\n",
    "        page_number = f\"### Page Number: [PG:{page_num}]\\n ### Source Document: {file_name}\\n\\n\"\n",
    "        contents.append(page_number + result + \"\\n\\n\\n\")\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segmentation(contents, segmentation, file_name):\n",
    "    pages = \"\\n\".join(contents).split(\"### Page Number:\")\n",
    "    pages = [\"### Page Number:\" + page for page in pages[1:]] \n",
    "    pages = [page.strip() for page in pages if page.strip()]\n",
    "\n",
    "    data = []\n",
    "    for section in segmentation:\n",
    "\n",
    "        page_content = \"\"\n",
    "        for page in pages[section['page_range']['start']-1:section['page_range']['end']]:\n",
    "            page_content += page + \"\\n\\n\"\n",
    "        \n",
    "        data.append({\n",
    "            \"heading\": section['heading'],\n",
    "            \"page_range\": section['page_range'],\n",
    "            \"description\": section['description'],\n",
    "            \"content\": page_content,\n",
    "            \"source\": file_name\n",
    "        })\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageRange(BaseModel):\n",
    "    start: int = Field(..., description=\"Starting page number\")\n",
    "    end: int = Field(..., description=\"Ending page number\")\n",
    "\n",
    "class Segment(BaseModel):\n",
    "    heading: str = Field(..., description=\"The heading or title of the section\")\n",
    "    description: str = Field(..., description=\"A brief description or summary of the section\")\n",
    "    page_range: PageRange = Field(..., description=\"Page range for this section\")\n",
    "    \n",
    "client = instructor.from_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_data, client):\n",
    "    i, text_chunk, step, start_page, end_page = chunk_data\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    You are an advanced financial analysis AI designed to segment financial documents for use in a Retrieval-Augmented Generation (RAG) system. Your goal is to analyze and divide the document into precise, meaningful sections that can be easily indexed and retrieved based on user queries.\n",
    "\n",
    "    Your objective is to divide this document into logical sections based on content and structure, ensuring complete coverage of all pages.\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    1. Read through the entire document carefully.\n",
    "    2. Identify logical sections based on the broader theme of the content and structure of the document.\n",
    "    3. For each section:\n",
    "        a. Determine the section's content and create an appropriate title.\n",
    "        b. Identify the page number range where the section is located.\n",
    "        c. Format the section with its title and page citation.\n",
    "    4. Ensure comprehensive coverage of the entire document, from page 1 through the final page.\n",
    "    5. The page number in the document is provided at the top of each page in the format \"### Page Number: [PG:X]\". Always refer to these page numbers in your response to accurately reference the source of information.\n",
    "    6. Do not rely on any index section or table of contents for page numbers. Always use the  \"### Page Number: [PG:X]\" format provided at the top of each page for accurate page references.\n",
    "    7. Output only factual, documentable information with accurate page references.\n",
    "    8. Make the sectioning as granular as possible, breaking down larger sections into smaller, more specific subsections where appropriate.\n",
    "    \"\"\"\n",
    "    \n",
    "    USER_PROMPT = f\"\"\"\n",
    "    Please provide a detailed segmentation of the legal document for pages {start_page} to {end_page}.\n",
    "\n",
    "    Important:\n",
    "    - The page number in the document is provided at the top of each page in the format \"### Page Number: [PG:X]\". Always refer to these page numbers in your response to accurately reference the source of information.\n",
    "\n",
    "    - Do not rely on any index section or table of contents for page numbers. Always use the  \"### Page Number: [PG:X]\" format provided at the top of each page for accurate page references.\n",
    "    - **Ensure that no pages are skipped or missed in the segmentation process. Every page must be accounted for in your response.**\n",
    "\n",
    "    <Document>\n",
    "        {text_chunk}\n",
    "    </Document>\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "            ],\n",
    "            temperature = 0.2,\n",
    "            response_model=List[Segment]\n",
    "        )\n",
    "        response_segments = response\n",
    "\n",
    "        chunk_segments = []\n",
    "        for segment in response_segments:\n",
    "            page_range = segment.page_range\n",
    "            chunk_segments.append({\n",
    "                \"heading\": segment.heading,\n",
    "                \"description\": segment.description,\n",
    "                \"page_range\": {\n",
    "                    \"start\": page_range.start,\n",
    "                    \"end\": page_range.end\n",
    "                }\n",
    "            })\n",
    "        return chunk_segments\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i//step + 1}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_content_instructor(content_array, client, chunk_size=200, overlap=20, max_workers=10):\n",
    "    all_segmentations = []\n",
    "    step = chunk_size - overlap\n",
    "\n",
    "    chunks = []\n",
    "    total_pages = len(content_array)\n",
    "    print(f\"Total pages: {total_pages}\")\n",
    "\n",
    "    for i in range(0, len(content_array), step):\n",
    "        chunk_end = min(i + chunk_size, len(content_array))\n",
    "        text_chunk = content_array[i:chunk_end]\n",
    "        start_page = (i // step) * (chunk_size - overlap) + 1\n",
    "        end_page = min(start_page + chunk_size - 1, total_pages)\n",
    "        \n",
    "        chunks.append((i, text_chunk, step, start_page, end_page))\n",
    "\n",
    "    ordered_results = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_chunk, chunk_data,client): chunk_data[0] for chunk_data in chunks}\n",
    "        \n",
    "        with tqdm(total=len(chunks), desc=\"Processing chunks\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                chunk_index = futures[future]\n",
    "                chunk_segments = future.result()\n",
    "                ordered_results[chunk_index] = chunk_segments\n",
    "                pbar.update(1)\n",
    "\n",
    "    for i in range(0, len(content_array), step):\n",
    "        if i in ordered_results:\n",
    "            all_segmentations.extend(ordered_results[i])\n",
    "    \n",
    "    return all_segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_document(segment_data):\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    Generate a concise description of the document based on its segments, focusing on main topics and themes.\n",
    "    \"\"\"\n",
    "\n",
    "    USER_PROMPT = f\"\"\"\n",
    "      Document Segments:\n",
    "      {segment_data}\n",
    "      Summarize the document in 100 words or less.\n",
    "    \"\"\"\n",
    "\n",
    "    response = completion(\n",
    "        model=\"openrouter/google/gemini-flash-1.5\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weaviate_client():\n",
    "    wcd_url = os.environ[\"WCD_URL\"]\n",
    "    wcd_api_key = os.environ[\"WCD_API_KEY\"]\n",
    "    openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    voyageai_api_key = os.environ[\"VOYAGEAI_API_KEY\"]\n",
    "\n",
    "    headers = {\n",
    "        \"X-VoyageAI-Api-Key\": voyageai_api_key,\n",
    "        \"X-OpenAI-Api-Key\": openai_api_key\n",
    "    }\n",
    "\n",
    "    client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=wcd_url,\n",
    "        auth_credentials=Auth.api_key(wcd_api_key),\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    if client.is_ready():\n",
    "        print(\"Weaviate client is ready\")\n",
    "        return client\n",
    "    else:\n",
    "        print(\"Weaviate client is not ready\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_get_collection(client, name, generative_model):\n",
    "    try:\n",
    "        print(\"Creating a new collection\", name)\n",
    "        return client.collections.create(\n",
    "            name = name,\n",
    "            vectorizer_config=[\n",
    "                Configure.NamedVectors.text2vec_openai(\n",
    "                    name=\"heading_description\",\n",
    "                    source_properties=[\"heading\", \"description\"], \n",
    "                    vector_index_config=Configure.VectorIndex.hnsw()\n",
    "                ),\n",
    "            ],\n",
    "            properties=[ \n",
    "                Property(name=\"heading\", data_type=DataType.TEXT),\n",
    "                Property(name=\"description\", data_type=DataType.TEXT),\n",
    "            ],\n",
    "            generative_config=Configure.Generative.openai(model=generative_model)\n",
    "            )\n",
    "\n",
    "    except weaviate.exceptions.UnexpectedStatusCodeException:\n",
    "        print(\"Collection already exists\")\n",
    "        return client.collections.get(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_to_collection(collection, data):\n",
    "    for document in data:\n",
    "        try:\n",
    "            heading = document['heading']\n",
    "            content = document['content']\n",
    "            page_range = document['page_range']\n",
    "            description = document['description']\n",
    "            source = document['source']\n",
    "\n",
    "            embed_client = OpenAI()\n",
    "\n",
    "            def get_embedding(text):\n",
    "                response = embed_client.embeddings.create(\n",
    "                    input= text,\n",
    "                    model=\"text-embedding-3-small\"\n",
    "                )\n",
    "                return response.data[0].embedding                \n",
    "\n",
    "            combined_text = heading + \" \" + description\n",
    "            combined_em = get_embedding(combined_text)\n",
    "\n",
    "            uuid = collection.data.insert(\n",
    "                properties={\n",
    "                    \"heading\": heading,\n",
    "                    \"description\": description,\n",
    "                    \"page_range\": page_range,\n",
    "                    \"content\": content,\n",
    "                    \"source\": source\n",
    "                },\n",
    "                vector={\n",
    "                    \"heading_description\": combined_em,\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting document: {str(e)}\")\n",
    "\n",
    "    print(f\"All documents have been added to the {collection.name} collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single File Flow\n",
    "\n",
    "# data_folder_path = os.path.join(os.getcwd(), 'Data')\n",
    "# file_name = \"3M_2021_10K.pdf\"\n",
    "\n",
    "# pdf_path = os.path.join(data_folder_path, file_name)\n",
    "# base64_images = pdf_to_base64_images(pdf_path)\n",
    "# len(base64_images)\n",
    "# summary = process_images_to_summary(base64_images)\n",
    "\n",
    "\n",
    "# for page_num, result in enumerate(summary, start=1):\n",
    "#     print(f\"Page Number: {page_num}\")\n",
    "#     print(result)\n",
    "#     print(\"*****\")\n",
    "#     print(\"\\n\\n\")\n",
    "\n",
    "# contents = prepare_data_for_ingestion(summary, file_name)\n",
    "\n",
    "# segments = process_content_instructor(contents, client, chunk_size=20, overlap=1, max_workers=10)\n",
    "# len(segments)\n",
    "\n",
    "# final_data = process_segmentation(contents, segments, file_name)\n",
    "# len(final_data)\n",
    "\n",
    "# formatted_segments = []\n",
    "# for i, segment in enumerate(segments):\n",
    "#     print(f\"Segment {i+1}:\")\n",
    "#     print(f\"Heading: {segment['heading']}\")\n",
    "#     print(f\"Description: {segment['description']}\")\n",
    "#     print(f\"Pages: {segment['page_range']['start']}-{segment['page_range']['end']}\")\n",
    "#     print(\"\\n\")\n",
    "#     formatted_segment = f\"Segment {i+1}:\\nHeading: {segment['heading']}\\nDescription: {segment['description']}\\nPages: {segment['page_range']['start']}-{segment['page_range']['end']}\\n\"\n",
    "#     formatted_segments.append(formatted_segment)\n",
    "    \n",
    "# formatted_segments = '\\n'.join(formatted_segments)\n",
    "\n",
    "# doc_classification = classify_document(formatted_segments)\n",
    "# print(doc_classification)\n",
    "\n",
    "# workspace_name = \"Test2\"\n",
    "\n",
    "# client_weaviate = get_weaviate_client()\n",
    "# collection = create_or_get_collection(client_weaviate, workspace_name, \"gpt-4o-mini\")\n",
    "# add_data_to_collection(collection, final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = os.path.join(os.getcwd(), 'Data')\n",
    "\n",
    "pdf_files = [f for f in os.listdir(data_folder_path) if \"AES\" in f.upper()]\n",
    "print(len(pdf_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in pdf_files[:7]:\n",
    "    print(f\"Processing {file_name}\")\n",
    "    pdf_path = os.path.join(data_folder_path, file_name)\n",
    "\n",
    "    base64_images = pdf_to_base64_images(pdf_path)\n",
    "    summary = process_images_to_summary(base64_images)\n",
    "    contents = prepare_data_for_ingestion(summary, file_name)\n",
    "\n",
    "    segments = process_content_instructor(contents, client, chunk_size=20, overlap=1, max_workers=10)\n",
    "    print(len(segments))\n",
    "\n",
    "    formatted_segments = []\n",
    "    for i, segment in enumerate(segments):\n",
    "        formatted_segment = f\"Segment {i+1}:\\nHeading: {segment['heading']}\\nDescription: {segment['description']}\\nPages: {segment['page_range']['start']}-{segment['page_range']['end']}\\n\"\n",
    "        formatted_segments.append(formatted_segment)\n",
    "    formatted_segments = '\\n'.join(formatted_segments)\n",
    "    doc_classification = classify_document(formatted_segments)\n",
    "\n",
    "    final_data = process_segmentation(contents, segments, file_name)\n",
    "    print(len(final_data))\n",
    "\n",
    "    workspace_name = \"Test2\"\n",
    "    client_weaviate = get_weaviate_client()\n",
    "    collection = create_or_get_collection(client_weaviate, workspace_name, \"gpt-4o-mini\")\n",
    "    add_data_to_collection(collection, final_data)\n",
    "    client_weaviate.close()\n",
    "\n",
    "    json_file_path = 'file_data.json'\n",
    "    try:\n",
    "        if os.path.exists(json_file_path):\n",
    "            with open(json_file_path, 'r') as json_file:\n",
    "                existing_data = json.load(json_file)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        existing_data.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"description\": doc_classification\n",
    "        })\n",
    "\n",
    "        with open(json_file_path, 'w') as json_file:\n",
    "            json.dump(existing_data, json_file, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
