{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "deepseek_client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    score: int = Field(..., description=\"Evaluation score between 0-10\", ge=0, le=10)\n",
    "    reasoning: str = Field(..., description=\"Detailed explanation of the evaluation and score justification\")\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('evaluation_results.json', 'r') as file:\n",
    "    evaluation_data = json.load(file)\n",
    "\n",
    "# for item in evaluation_data:\n",
    "#     question_number = item['question_number']\n",
    "#     question = item['question']\n",
    "#     reference_answer = item['reference_answer']\n",
    "#     generated_answer = item['generated_answer']\n",
    "\n",
    "#     print(f\"Question {question_number}:\")\n",
    "#     print(f\"Question: {question}\")\n",
    "#     print(f\"Reference Answer: {reference_answer}\")\n",
    "#     print(f\"Generated Answer: {generated_answer}\")\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_assesment(data):\n",
    "    try: \n",
    "        prompt = f\"\"\"\n",
    "        You are an expert evaluator specializing in assessing RAG (Retrieval-Augmented Generation) system responses. Your role is to evaluate both the retrieval accuracy and the quality of generated answers.\n",
    "\n",
    "        Given the following:\n",
    "            {data}\n",
    "\n",
    "        Please provide:\n",
    "        1. A score between 0-10 based on the following criteria:\n",
    "        - 0-3: Poor retrieval and/or incorrect information generation\n",
    "        - 4-6: Partial retrieval with some relevant information but gaps or inaccuracies\n",
    "        - 7-8: Good retrieval with mostly accurate information and minor inconsistencies\n",
    "        - 9-10: Excellent retrieval and generation with comprehensive and correct information\n",
    "\n",
    "        2. Detailed reasoning for the score, considering:\n",
    "        - **Information Presence**: Does the response contain the key information required?\n",
    "        - **Information Accuracy**: Is the retrieved information factually correct?\n",
    "        - **Generation Quality**: Is the response well-structured and coherent?\n",
    "        - **Hallucination Check**: Does the response introduce any incorrect or fabricated details?\n",
    "        - **Context Utilization**: How effectively is the retrieved information incorporated into the response?\n",
    "\n",
    "        Focus on whether the necessary information is included and accurate rather than requiring an exact match with a reference answer.\n",
    "\n",
    "        Format your response as:\n",
    "        Score: [number]  \n",
    "        Reasoning: [your detailed RAG system evaluation]\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"o1-mini\",\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "        )\n",
    "        assesment = response.choices[0].message.content\n",
    "        return assesment\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"Error: Could not generate assesment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_evaluation(llm_response):\n",
    "    try:\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        Your role is to take the assesment and structure it in a format that is easy to read and understand.\n",
    "\n",
    "        Please provide the assesment in the following format.\n",
    "        \"\"\"\n",
    "\n",
    "        USER_PROMPT = f\"\"\"\n",
    "        Assesment: {llm_response}\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "            ],\n",
    "            temperature = 0.2,\n",
    "            response_model=Evaluation\n",
    "        )\n",
    "        return response\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Could not parse LLM response: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in evaluation_data[0:]:\n",
    "    eval_string = f\"\"\"\n",
    "    <question>{item['question']}</question>\n",
    "    <reference_answer>{item['reference_answer']}</reference_answer>\n",
    "    <generated_answer>{item['generated_answer']}</generated_answer>\n",
    "    \"\"\"\n",
    "    assesment = provide_assesment(eval_string)\n",
    "    evaluation = process_evaluation(assesment)\n",
    "\n",
    "    score = evaluation.score\n",
    "    reasoning = evaluation.reasoning\n",
    "    print(score)\n",
    "    item['reasoning'] = reasoning\n",
    "    item['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('evaluation.json', 'w') as file:\n",
    "    json.dump(evaluation_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_score(evaluations: list[Evaluation]) -> float:\n",
    "    if not evaluations:\n",
    "        return 0.0\n",
    "        \n",
    "    total_score = sum(eval.score for eval in evaluations)\n",
    "    average = total_score / len(evaluations)\n",
    "    return round(average, 2)\n",
    "\n",
    "results = []\n",
    "for item in evaluation_data:\n",
    "    score = item['score']\n",
    "    reasoning = item['reasoning']\n",
    "    results.append(Evaluation(score=score, reasoning=reasoning))\n",
    "\n",
    "average_score = calculate_average_score(results)\n",
    "print(f\"Average RAG System Score: {average_score}/10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
