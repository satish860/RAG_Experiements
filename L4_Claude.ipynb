{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32955c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q litellm tqdm PyMuPDF Pillow instructor openai python-dotenv weaviate-client pandas anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "956b9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import Filter\n",
    "from litellm import completion\n",
    "import re\n",
    "from anthropic import Anthropic\n",
    "import pandas as pd\n",
    "\n",
    "anthropic_client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "client = OpenAI()\n",
    "O1_MODEL = 'o1-mini'\n",
    "GPT_MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f81a7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query):\n",
    "    client = OpenAI(api_key=os.environ[\"PERPLEXITYAI_API_KEY\"], base_url=\"https://api.perplexity.ai\")\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an artificial intelligence assistant and you need to \"\n",
    "                \"answer like a Data collection engine with as much information as possible.\"\n",
    "            ),\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"sonar\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    citations = response.citations\n",
    "\n",
    "    def replace_citation_with_link(match):\n",
    "        citation_num_str = match.group(0)[1:-1]\n",
    "        citation_idx = int(citation_num_str) - 1\n",
    "        \n",
    "        if citation_idx < 0 or citation_idx >= len(citations):\n",
    "            return match.group(0)\n",
    "        return f\"({citations[citation_idx]})\"\n",
    "    \n",
    "    content_with_links = re.sub(r'\\[\\d+\\]', replace_citation_with_link, content)\n",
    "    return content_with_links\n",
    "\n",
    "\n",
    "def calculate(num1: float, num2: float, operator: str) -> float:\n",
    "        operators = {\n",
    "            '+': lambda x, y: x + y,\n",
    "            '-': lambda x, y: x - y,\n",
    "            '*': lambda x, y: x * y,\n",
    "            '/': lambda x, y: x / y if y != 0 else raise_(ZeroDivisionError(\"Cannot divide by zero.\"))\n",
    "        }\n",
    "        if operator not in operators:\n",
    "            raise ValueError(\"Invalid operator. Expected one of '+', '-', '*', '/'.\")\n",
    "    \n",
    "        return str(operators[operator](num1, num2))\n",
    "\n",
    "def raise_(ex):\n",
    "    \"\"\"Helper function to raise exceptions in lambda functions.\"\"\"\n",
    "    raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5679a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weaviate_client():\n",
    "    wcd_url = os.environ[\"WCD_URL\"]\n",
    "    wcd_api_key = os.environ[\"WCD_API_KEY\"]\n",
    "    openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    voyageai_api_key = os.environ[\"VOYAGEAI_API_KEY\"]\n",
    "\n",
    "    headers = {\n",
    "        \"X-VoyageAI-Api-Key\": voyageai_api_key,\n",
    "        \"X-OpenAI-Api-Key\": openai_api_key\n",
    "    }\n",
    "\n",
    "    client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=wcd_url,\n",
    "        auth_credentials=Auth.api_key(wcd_api_key),\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    if client.is_ready():\n",
    "        print(\"Weaviate client is ready\")\n",
    "        return client\n",
    "    else:\n",
    "        print(\"Weaviate client is not ready\")\n",
    "        return None\n",
    "\n",
    "workspace_name = \"Test2\"\n",
    "weaviate_client = get_weaviate_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f21c176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file_data.json\", 'r') as json_file:\n",
    "    existing_data = json.load(json_file)\n",
    "\n",
    "formatted_content = \"\"\n",
    "for item in existing_data:\n",
    "    formatted_content += f\"**{item['file_name']}**: {item['description']}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3fecabdc-9fea-4e71-bf61-ad56c2ee45c2",
   "metadata": {
    "height": 1032
   },
   "outputs": [],
   "source": [
    "o1_prompt = f\"\"\"\n",
    "You are a financial analysis assistant. The first input you will receive will be a complex financial task that needs to be carefully reasoned through to solve. \n",
    "Your task is to review the challenge, and ccreate a focused and concise plan to analyze financial statements, assess risks, and provide insights.\n",
    "\n",
    "You will have access to an LLM agent that is responsible for executing the plan that you create and will return results.\n",
    "\n",
    "The LLM agent has access to the following functions:\n",
    "    - fetch_context(Question, file_name)\n",
    "        - The function fetches context data from relevant financial statements based on the provided question, which should include the file name.\n",
    "    - search_web(query)\n",
    "        - This function has access to the latest information from the web and can be used to fetch industry benchmarks and various other data that cannot be found in the statement documents.\n",
    "\n",
    "**Available Files:**\n",
    "    {formatted_content}\n",
    "\n",
    "When creating a plan for the LLM to execute, break your instructions into a logical, step-by-step order, using the specified format:\n",
    "    - **Main actions are numbered** (e.g., 1, 2, 3).\n",
    "    - **Sub-actions are lettered** under their relevant main actions (e.g., 1a, 1b).\n",
    "        - **Sub-actions should start on new lines**\n",
    "    - **Specify conditions using clear 'if...then...else' statements** (e.g., 'If the financial statement shows a profit, then...').\n",
    "    - **For actions that require using one of the above functions defined**, write a step to call a function using backticks for the function name (e.g., `call the fetch_context function`).\n",
    "        - Ensure that the proper input arguments are given to the model for instruction. There should not be any ambiguity in the inputs.\n",
    "    - **The last step** in the instructions should always be calling the `instructions_complete` function. This is necessary so we know the LLM has completed all of the instructions you have given it.\n",
    "    - **Detailed steps** The plan generated must be extremely detailed and thorough with explanations at every step.\n",
    "    - **Response Depth Analysis**\n",
    "        - Provide only the requested information when the user asks for simple identification or specific numbers, avoiding any additional analysis or interpretation. \n",
    "\n",
    "    <example>\n",
    "    <scenario>\n",
    "    What was the percentage contribution of the healthcare segment to 3M's total revenue in 2022?\n",
    "    </scenario>\n",
    "    <response>\n",
    "        # Financial Analysis Plan to Calculate Healthcare Segment's Revenue Contribution in 2022\n",
    "        ## 1. Gather Relevant Financial Data  \n",
    "        a. `fetch_context(Question=\"Provide total revenue for 3M in 2022.\", file_name=\"3M_2022_10K.pdf\")`  \n",
    "        b. `fetch_context(Question=\"Provide revenue for the healthcare segment in 2022.\", file_name=\"3M_2022_10K.pdf\")`  \n",
    "            \n",
    "        ## 2. Calculate Percentage Contribution  \n",
    "        a. Divide the healthcare segment's revenue by the total revenue for 2022:   \n",
    "        b. Multiply the result by 100 to express the contribution as a percentage:  \n",
    "\n",
    "        ## 3. Finalize and Complete Instructions  \n",
    "        - `instructions_complete()`  \n",
    "    </response>\n",
    "    </example>\n",
    "Use markdown format when generating the plan with each step and sub-step.\n",
    "Please find the scenario below.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9ff5752-acb0-4bb1-adb3-58ce2ac1133f",
   "metadata": {
    "height": 335
   },
   "outputs": [],
   "source": [
    "claude_system_prompt = \"\"\"\n",
    "You are a helpful assistant responsible for executing the policy on handling financial analysis tasks. \n",
    "Your task is to follow the policy exactly as it is written and perform the necessary actions.\n",
    "\n",
    "You must explain your decision-making process across various steps.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Read and Understand Policy**: Carefully read and fully understand the given policy on handling financial analysis tasks.\n",
    "2. **Identify the exact step in the policy**: Determine which step in the policy you are at, and execute the instructions according to the policy.\n",
    "3. **Decision Making**: Briefly explain your actions and why you are performing them.\n",
    "4. **Action Execution**: Perform the actions required by calling any relevant functions and input parameters.\n",
    "5. **Use of External Knowledge**: If specific industry trends or benchmark data are required and not available in the policy or context, use search_web function to fetch information.\n",
    "\n",
    "POLICY:\n",
    "{policy}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa6c37dd-8ec5-4ed7-9cfd-0259867cd4fd",
   "metadata": {
    "height": 4925
   },
   "outputs": [],
   "source": [
    "TOOLS = [\n",
    "    {\n",
    "        \"name\": \"fetch_context\",\n",
    "        \"description\": \"Fetches context data from relevant financial statements based on the provided question.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"Question\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The question to fetch context data for.\"\n",
    "                },\n",
    "                \"file_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Name of the file to fetch the context from.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"Question\", \"file_name\"]\n",
    "        }\n",
    "    },   \n",
    "    {\n",
    "        \"name\": \"search_web\",\n",
    "        \"description\": \"Function performs a web search and returns relevant information based on the provided query\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The search query to be processed\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform basic arithmetic calculations between two numbers\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"num1\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number in the calculation\"\n",
    "                },\n",
    "                \"num2\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number in the calculation\"\n",
    "                },\n",
    "                \"operator\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The arithmetic operator to use\",\n",
    "                    \"enum\": [\"+\", \"-\", \"*\", \"/\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"num1\", \"num2\", \"operator\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"instructions_complete\",\n",
    "        \"description\": \"Function should be called when we have completed ALL of the instructions.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"final_report\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Final Report based on the analysis.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"final_report\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "130643d7-3043-45b9-baec-a1e37db91c97",
   "metadata": {
    "height": 1882
   },
   "outputs": [],
   "source": [
    "def fetch_context(Question, file_name):\n",
    "    try:\n",
    "        questions = weaviate_client.collections.get(workspace_name)\n",
    "\n",
    "        response = questions.query.hybrid(\n",
    "            query=Question,\n",
    "            limit=15,\n",
    "            alpha=0.5,\n",
    "            filters=Filter.by_property(\"source\").equal(file_name)\n",
    "        )\n",
    "\n",
    "        def weaviate_objects_to_text(objects):\n",
    "            text = \"\"\n",
    "            for obj in objects:\n",
    "                text += f\"<Content>\\n{obj.properties['content']}\\n</Content>\\n\\n\"\n",
    "            return text\n",
    "        \n",
    "        llm_input = weaviate_objects_to_text(response.objects)\n",
    "\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        You are a specialized Financial Analysis AI focused on extracting concise, data-driven insights from financial documents. \n",
    "        Provide short, precise answers with a brief explanation.\n",
    "\n",
    "        **Remember**:  \n",
    "        - Stick strictly to information from the documents.  \n",
    "        - Keep responses focused on numerical data and short insights.  \n",
    "        - Avoid speculation or overly detailed elaboration.  \n",
    "        - Provide a brief but clear thinking process, then a concise final answer.\n",
    "        \"\"\"\n",
    "\n",
    "        USER_PROMPT = f\"\"\"\n",
    "        Context:\n",
    "        <Context>\n",
    "            {llm_input}\n",
    "        </Context>\n",
    "\n",
    "        Question: \n",
    "        <Question>\n",
    "            {Question}\n",
    "        </Question>\n",
    "\n",
    "        Please provide a concise, reader-friendly answer.\n",
    "        - Provide direct, concise answers suitable for FAQ format\n",
    "        - Focus on accuracy and readability\n",
    "        - Include only information found in the provided context\n",
    "        - If information is incomplete or unclear, state this explicitly\n",
    "        \"\"\"\n",
    "\n",
    "        response = completion(\n",
    "            model=\"openrouter/google/gemini-flash-1.5\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return f\"An error occurred while processing: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "848828c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_mapping = {\n",
    "    'fetch_context': fetch_context,\n",
    "    'search_web': search_web,\n",
    "    'calculate': calculate,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a8442ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_message(message_list, message):\n",
    "    message_list.append(message)\n",
    "    message_type = message.get('type', '')\n",
    "    if message_type == 'status':\n",
    "        print(message['message'])\n",
    "    elif message_type == 'plan':\n",
    "        print(\"\\nPlan:\\n\", message['content'])\n",
    "    elif message_type == 'assistant':\n",
    "        print(\"\\nAssistant:\\n\", message['content'])\n",
    "    elif message_type == 'function_call':\n",
    "        print(f\"\\nFunction call: {message['function_name']} with arguments {message['arguments']}\")\n",
    "    elif message_type == 'function_response':\n",
    "        print(f\"\\nFunction response for {message['function_name']}: {message['response']}\")\n",
    "    else:\n",
    "        print(message.get('content', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9fb85498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_o1(scenario):\n",
    "    prompt = f\"\"\"\n",
    "    {o1_prompt}\n",
    "        \n",
    "    Scenario:\n",
    "    {scenario}\n",
    "\n",
    "    Please provide the next steps in your plan.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=O1_MODEL,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    plan = response.choices[0].message.content\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a94c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_anthropic(message_list, plan):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': plan}\n",
    "    ]\n",
    "    \n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=1024,\n",
    "        system=claude_system_prompt,\n",
    "        tools=TOOLS,\n",
    "        messages=messages\n",
    "    )\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "    final_answer = \"\"\n",
    " \n",
    "    while response.stop_reason == \"tool_use\":\n",
    "        if response.content and len(response.content) > 0:\n",
    "            text_block = response.content[0]\n",
    "            if hasattr(text_block, 'text') and text_block.text:\n",
    "                print(\"\\nModel Response:\", text_block.text)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        tool = response.content[-1]\n",
    "        print(\"\\nCalling Tool:\", tool.name)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        if (tool.name and tool.name == 'instructions_complete'):\n",
    "            final_answer = tool.input\n",
    "            print(\"\\n\", final_answer)\n",
    "            print(\"-\" * 80)\n",
    "            break\n",
    "\n",
    "        if tool.name in function_mapping:\n",
    "            input_arguments_str = tool.input\n",
    "            print(\"\\nInput Arguments:\", input_arguments_str)\n",
    "            print(\"-\" * 80)\n",
    "            res = function_mapping[tool.name](**input_arguments_str)\n",
    "            print(\"\\nTool Response:\", res)\n",
    "            print(\"-\" * 80)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tool: {tool.name}\")\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": [{\n",
    "            \"type\": \"tool_result\",\n",
    "            \"tool_use_id\": tool.id,\n",
    "            \"content\": res\n",
    "        }]})\n",
    "\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=1024,\n",
    "            system=claude_system_prompt,\n",
    "            tools=TOOLS,\n",
    "            messages=messages\n",
    "        )\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "\n",
    "    return messages, final_answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57bcf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenario(message_list, scenario):\n",
    "    append_message(message_list, {'type': 'status', 'message': 'Generating plan...'})\n",
    "    plan = call_o1(scenario)\n",
    "    print(\"-\" * 80)\n",
    "    append_message(message_list, {'type': 'plan', 'content': plan})\n",
    "    print(\"-\" * 80)\n",
    "    append_message(message_list, {'type': 'status', 'message': 'Executing plan...'})\n",
    "    print(\"-\" * 80)\n",
    "    messages, answer = call_anthropic(message_list, plan)\n",
    "    append_message(message_list, {'type': 'status', 'message': 'Processing complete.'})\n",
    "    return messages, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f000b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario_text = (\"What is the FY2019 fixed asset turnover ratio for Activision Blizzard? Fixed asset turnover ratio is defined as: FY2019 revenue / (average PP&E between FY2018 and FY2019). Round your answer to two decimal places. Base your judgments on the information provided primarily in the statement of income and the statement of financial position.\")\n",
    "\n",
    "# message_list = []\n",
    "# messages = process_scenario(message_list, scenario_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_json(\"financebench_open_source.jsonl\", lines=True)\n",
    "filtered_df = df_questions[df_questions[\"doc_name\"].str.contains(\"AES|ADOBE|ACTIVISIONBLIZZARD|AMAZON|AMCOR\", case=False, na=False)]\n",
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a7d7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03038c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in filtered_df.iterrows():\n",
    "    print(f\"\\nQuestion {i+1}:\")\n",
    "    print(\"Q:\", row['question'])\n",
    "    scenario_text = (row['question'])\n",
    "    message_list = []\n",
    "    messages, answer = process_scenario(message_list, scenario_text)\n",
    "    answers.append(answer)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 8\n",
    "# for idx, row in filtered_df.iloc[8:].iterrows(): \n",
    "#     print(f\"\\nQuestion {i+1}:\")\n",
    "#     print(\"Q:\", row['question'])\n",
    "#     scenario_text = (row['question'])\n",
    "#     message_list = []\n",
    "#     messages, answer = process_scenario(message_list, scenario_text)\n",
    "#     answers.append(answer)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    print(f\"\\nQuestion {i+1}:\")\n",
    "    print(\"Q:\", row['question'])\n",
    "    print(\"Reference Answer:\", row['answer'])\n",
    "    # print(row['justification'], row['evidence'])\n",
    "\n",
    "    print(\"Generated Answer:\", answers[i])\n",
    "    i = i + 1\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []  \n",
    "\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    print(f\"\\nQuestion {i+1}:\")\n",
    "    print(\"Q:\", row['question'])\n",
    "    print(\"Reference Answer:\", row['answer'])\n",
    "    print(\"Generated Answer:\", answers[i])\n",
    "\n",
    "    if isinstance(answers[i], dict):\n",
    "        final_report = answers[i].get('final_report', str(answers[i]))\n",
    "    else:\n",
    "        try:\n",
    "            generated_answer = json.loads(answers[i])\n",
    "            final_report = generated_answer.get('final_report', answers[i])\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            final_report = answers[i] \n",
    "\n",
    "    result_dict = {\n",
    "        \"question_number\": i + 1,\n",
    "        \"question\": row['question'],\n",
    "        \"reference_answer\": row['answer'],\n",
    "        \"generated_answer\": final_report\n",
    "    }\n",
    "    results.append(result_dict)\n",
    "    \n",
    "    i = i + 1\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfcc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
